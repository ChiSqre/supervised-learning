{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning for Energy Efficiency\n",
    "Author: Andrew Farell\n",
    "\n",
    "In this project, I focus on predicting **heating and cooling loads** for residential buildings. My objective is to explore how different building features (like shape, surface area, and glazing) affect energy demands. **If I am successful, this model could help designers and architects optimize for efficiency.**\n",
    "\n",
    "---\n",
    "\n",
    "## Problem Definition\n",
    "\n",
    "I’m working with the **ENB2012 dataset**, which includes 768 building configurations simulated in Ecotect. Each configuration has eight features (X1–X8) and two target variables:\n",
    "\n",
    "- **X1**: Relative Compactness  \n",
    "- **X2**: Surface Area  \n",
    "- **X3**: Wall Area  \n",
    "- **X4**: Roof Area  \n",
    "- **X5**: Overall Height  \n",
    "- **X6**: Orientation  \n",
    "- **X7**: Glazing Area  \n",
    "- **X8**: Glazing Area Distribution  \n",
    "\n",
    "The two targets are:\n",
    "\n",
    "1. **Y1** (Heating Load)  \n",
    "2. **Y2** (Cooling Load)  \n",
    "\n",
    "This is a supervised learnign task, i.e. I am trying to predict Y1 and Y2 from X1-X8.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "I'm going to first explore the data to see how the features and targets behave. The outputs of this analysis are in the `outputs/` folder, along with a file called `analysis.json` that has key statistical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDA completed successfully. Results saved to 'outputs/' directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_excel(\"ENB2012_data.xlsx\")\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "numeric_describe = data.describe().T\n",
    "\n",
    "missing_summary = data.isna().sum().to_dict()\n",
    "\n",
    "dtypes_summary = {col: str(dtype) for col, dtype in data.dtypes.items()}\n",
    "\n",
    "corr = data.corr()\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Heatmap of All Features and Targets\")\n",
    "plt.tight_layout()\n",
    "heatmap_path = os.path.join(\"outputs\", \"correlation_heatmap.png\")\n",
    "plt.savefig(heatmap_path, dpi=300)\n",
    "plt.close()\n",
    "\n",
    "for col in data.columns:\n",
    "    plt.figure()\n",
    "    sns.histplot(data[col], kde=True, color=\"blue\", edgecolor=\"black\")\n",
    "    plt.title(f\"Histogram of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.tight_layout()\n",
    "    hist_path = os.path.join(\"outputs\", f\"{col}_hist.png\")\n",
    "    plt.savefig(hist_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "target_columns = [\"Y1\", \"Y2\"]\n",
    "for target in target_columns:\n",
    "    plt.figure()\n",
    "    sns.boxplot(y=data[target], color=\"green\")\n",
    "    plt.title(f\"Box Plot of {target}\")\n",
    "    plt.ylabel(target)\n",
    "    plt.tight_layout()\n",
    "    boxplot_path = os.path.join(\"outputs\", f\"{target}_boxplot.png\")\n",
    "    plt.savefig(boxplot_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "analysis_dict = {\n",
    "    \"numeric_describe\": numeric_describe.to_dict(),\n",
    "    \"missing_values\": missing_summary,\n",
    "    \"data_types\": dtypes_summary,\n",
    "    \"correlation_matrix\": corr.to_dict()\n",
    "}\n",
    "\n",
    "analysis_path = os.path.join(\"outputs\", \"analysis.json\")\n",
    "with open(analysis_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(analysis_dict, f, indent=4)\n",
    "\n",
    "print(\"EDA completed successfully. Results saved to 'outputs/' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Integrity and Basic Statistics\n",
    "\n",
    "- **No Missing Values**: I confirmed that none of the 768 rows contain missing data.  \n",
    "- **Data Types**: Most features are floats (e.g., X1, X2, etc.), while X6 and X8 are integers. This matches the dataset’s official description.  \n",
    "- **Descriptive Stats**: Here are some highlights (more details can be found in `analysis.json`):  \n",
    "  - **Relative Compactness (X1)** varies from 0.62 to 0.98 (mean ~0.76).  \n",
    "  - **Surface Area (X2)** ranges from 514.5 to 808.5, averaging around 671.71.  \n",
    "  - **Heating Load (Y1)** spans 6.01 to 43.1 (mean ~22.31).  \n",
    "  - **Cooling Load (Y2)** goes from 10.9 to 48.03 (mean ~24.59).  \n",
    "\n",
    "### 2. Univariate Distributions\n",
    "\n",
    "I created histograms for each feature and target to visualize their distributions. For example:\n",
    "\n",
    "![Histogram of X1](outputs/X1_hist.png)\n",
    "\n",
    "Similar images for X2 through Y2 (e.g., `X2_hist.png`, `Y1_hist.png`, etc.) are located in the `outputs/` directory. These plots help me see how each variable is spread out (e.g., X1 peaks near 0.75, X2 clusters around ~670 m²).\n",
    "\n",
    "### 3. Target Variable Box Plots\n",
    "\n",
    "To spot any outliers, I also plotted box plots for **Heating Load (Y1)** and **Cooling Load (Y2)**:\n",
    "\n",
    "![Y1 Box Plot](outputs/Y1_boxplot.png)\n",
    "![Y2 Box Plot](outputs/Y2_boxplot.png)\n",
    "\n",
    "The box plots indicate that a subset of buildings require substantially higher energy, which likely ties to differences in surface area, height, orientation, or other design aspects.\n",
    "\n",
    "### 4. Correlation Analysis\n",
    "\n",
    "Finally, I examined feature correlations and how they relate to the targets:\n",
    "\n",
    "![Correlation Heatmap](outputs/correlation_heatmap.png)\n",
    "\n",
    "Notable observations include:\n",
    "- **X5 (Overall Height)** has a strong positive correlation with both Y1 (~0.89) and Y2 (~0.90). Taller buildings often need more energy for heating and cooling.  \n",
    "- **X4 (Roof Area)** is strongly negatively correlated with both Y1 and Y2 (~-0.86 each). This likely reflects interactions between building height, roof surface, and thermal transfers.  \n",
    "- **X1 (Relative Compactness)** also correlates well with Y1 and Y2, suggesting that more compact designs can reduce energy demands.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Results\n",
    "\n",
    "### 1. Model Definitions\n",
    "\n",
    "I tested two types of models:\n",
    "\n",
    "1. **Ridge Regression**  \n",
    "   This is a linear regression model with $\\ell_2$ regularization. I used a pipeline to **scale the features** and fit a Ridge regressor, exploring multiple values of the regularization parameter $\\alpha$.\n",
    "\n",
    "2. **XGBoost**  \n",
    "   XGBoost is a gradient boosting library that can handle complex, nonlinear relationships. I tuned:\n",
    "   - **learning_rate**: A step size parameter that helps avoid overfitting,  \n",
    "   - **max_depth**: The maximum depth of each decision tree,  \n",
    "   - **n_estimators**: The total number of boosting rounds (trees).\n",
    "\n",
    "### 2. Loss Function and Metrics\n",
    "\n",
    "Both models aim to **minimize Mean Squared Error (MSE)**:\n",
    "\n",
    "$$\n",
    "\\mathrm{MSE} \\;=\\; \\frac{1}{n}\\sum_{i=1}^n (\\hat{y}_i - y_i)^2,\n",
    "$$\n",
    "\n",
    "where $y_i$ is the true heating or cooling load, and $\\hat{y}_i$ is my model’s prediction. I used **5-fold cross-validation** to find the best hyperparameters and then evaluated final performance on a separate test set.\n",
    "\n",
    "### 3. Training Procedure\n",
    "\n",
    "1. **Data Splitting**: I reserved 20% of the data for final testing to ensure the models don’t overfit by seeing all samples during training.  \n",
    "2. **Scaling and Fitting**: I standardized the features (subtracting mean, dividing by std) and then trained each model on the remaining 80% of data.  \n",
    "3. **Hyperparameter Grid Search**: I systematically varied $\\alpha$ (for Ridge) and learning_rate, max_depth, n_estimators (for XGBoost) across a range of values. The combination that achieved the lowest cross-validation MSE was then refit on the full training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All model results saved to 'ridge_results.csv' and 'xgb_results.csv'.\n",
      "Learning curves saved to 'outputs/'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import clone\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "df = pd.read_excel(\"ENB2012_data.xlsx\")\n",
    "X = df[[\"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X7\", \"X8\"]]\n",
    "y1 = df[\"Y1\"]\n",
    "y2 = df[\"Y2\"]\n",
    "\n",
    "X_train, X_test, y1_train, y1_test = train_test_split(\n",
    "    X, y1, test_size=0.2, random_state=42\n",
    ")\n",
    "_, _, y2_train, y2_test = train_test_split(\n",
    "    X, y2, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "ridge_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"ridge\", Ridge())\n",
    "])\n",
    "ridge_params = {\n",
    "    \"ridge__alpha\": [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "}\n",
    "\n",
    "xgb_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"xgb\", XGBRegressor(objective=\"reg:squarederror\"))\n",
    "])\n",
    "xgb_params = {\n",
    "    \"xgb__learning_rate\": [0.01, 0.1],\n",
    "    \"xgb__max_depth\": [3, 5],\n",
    "    \"xgb__n_estimators\": [100, 200]\n",
    "}\n",
    "\n",
    "ridge_results_df = pd.DataFrame()\n",
    "xgb_results_df = pd.DataFrame()\n",
    "\n",
    "def full_grid_search(pipeline, param_grid, X_tr, y_tr, X_te, y_te, model_name, target_name):\n",
    "    grid = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=5,\n",
    "        verbose=0,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid.fit(X_tr, y_tr)\n",
    "\n",
    "    cv_res = grid.cv_results_\n",
    "\n",
    "    rows = []\n",
    "    for i, param_combo in enumerate(cv_res[\"params\"]):\n",
    "        row_dict = {}\n",
    "        row_dict[\"model\"] = model_name\n",
    "        row_dict[\"target\"] = target_name\n",
    "\n",
    "        for k, v in param_combo.items():\n",
    "            row_dict[k] = v\n",
    "\n",
    "        mean_cv_neg_mse = cv_res[\"mean_test_score\"][i]\n",
    "        std_cv_neg_mse = cv_res[\"std_test_score\"][i]\n",
    "        rank = cv_res[\"rank_test_score\"][i]\n",
    "\n",
    "        row_dict[\"mean_cv_mse\"] = -mean_cv_neg_mse\n",
    "        row_dict[\"std_cv_mse\"] = std_cv_neg_mse\n",
    "        row_dict[\"rank_test_score\"] = rank\n",
    "\n",
    "        pipe_copy = clone(pipeline)\n",
    "        pipe_copy.set_params(**param_combo)\n",
    "        pipe_copy.fit(X_tr, y_tr)\n",
    "        preds = pipe_copy.predict(X_te)\n",
    "        test_mse = np.mean((preds - y_te) ** 2)\n",
    "        row_dict[\"test_mse\"] = test_mse\n",
    "\n",
    "        rows.append(row_dict)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "ridge_y1_df = full_grid_search(\n",
    "    ridge_pipeline, ridge_params,\n",
    "    X_train, y1_train, X_test, y1_test,\n",
    "    \"Ridge\", \"Y1\"\n",
    ")\n",
    "ridge_results_df = pd.concat([ridge_results_df, ridge_y1_df], ignore_index=True)\n",
    "\n",
    "ridge_y2_df = full_grid_search(\n",
    "    ridge_pipeline, ridge_params,\n",
    "    X_train, y2_train, X_test, y2_test,\n",
    "    \"Ridge\", \"Y2\"\n",
    ")\n",
    "ridge_results_df = pd.concat([ridge_results_df, ridge_y2_df], ignore_index=True)\n",
    "\n",
    "xgb_y1_df = full_grid_search(\n",
    "    xgb_pipeline, xgb_params,\n",
    "    X_train, y1_train, X_test, y1_test,\n",
    "    \"XGBoost\", \"Y1\"\n",
    ")\n",
    "xgb_results_df = pd.concat([xgb_results_df, xgb_y1_df], ignore_index=True)\n",
    "\n",
    "xgb_y2_df = full_grid_search(\n",
    "    xgb_pipeline, xgb_params,\n",
    "    X_train, y2_train, X_test, y2_test,\n",
    "    \"XGBoost\", \"Y2\"\n",
    ")\n",
    "xgb_results_df = pd.concat([xgb_results_df, xgb_y2_df], ignore_index=True)\n",
    "\n",
    "ridge_results_path = os.path.join(\"outputs\", \"ridge_results.csv\")\n",
    "xgb_results_path = os.path.join(\"outputs\", \"xgb_results.csv\")\n",
    "\n",
    "ridge_results_df.to_csv(ridge_results_path, index=False)\n",
    "xgb_results_df.to_csv(xgb_results_path, index=False)\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def get_best_params(df_results):\n",
    "    best_idx = df_results[\"mean_cv_mse\"].idxmin()\n",
    "    best_row = df_results.loc[best_idx].to_dict()\n",
    "    param_dict = {}\n",
    "    for k, v in best_row.items():\n",
    "        if k.startswith(\"ridge__\") or k.startswith(\"xgb__\"):\n",
    "            param_dict[k] = v\n",
    "    return param_dict\n",
    "\n",
    "def plot_learning_curves(estimator, X, y, title):\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        estimator, X, y,\n",
    "        cv=5, scoring=\"neg_mean_squared_error\",\n",
    "        train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "        shuffle=True, random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    train_mse = -train_scores\n",
    "    val_mse = -val_scores\n",
    "\n",
    "    train_mse_mean = np.mean(train_mse, axis=1)\n",
    "    train_mse_std = np.std(train_mse, axis=1)\n",
    "    val_mse_mean = np.mean(val_mse, axis=1)\n",
    "    val_mse_std = np.std(val_mse, axis=1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_sizes, train_mse_mean, marker='o', label='Training MSE')\n",
    "    plt.fill_between(train_sizes,\n",
    "                        train_mse_mean - train_mse_std,\n",
    "                        train_mse_mean + train_mse_std,\n",
    "                        alpha=0.2)\n",
    "    plt.plot(train_sizes, val_mse_mean, marker='s', label='Validation MSE')\n",
    "    plt.fill_between(train_sizes,\n",
    "                        val_mse_mean - val_mse_std,\n",
    "                        val_mse_mean + val_mse_std,\n",
    "                        alpha=0.2)\n",
    "\n",
    "    plt.title(f\"Learning Curves: {title}\")\n",
    "    plt.xlabel(\"Training Set Size\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "best_ridge_y1 = get_best_params(ridge_results_df[ridge_results_df[\"target\"]==\"Y1\"])\n",
    "ridge_pipeline_best_y1 = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"ridge\", Ridge())\n",
    "])\n",
    "ridge_pipeline_best_y1.set_params(**best_ridge_y1)\n",
    "\n",
    "plot_learning_curves(ridge_pipeline_best_y1, X_train, y1_train, \"Ridge (Best) - Y1\")\n",
    "plt.savefig(os.path.join(\"outputs\", \"learning_curve_ridge_y1.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "best_ridge_y2 = get_best_params(ridge_results_df[ridge_results_df[\"target\"]==\"Y2\"])\n",
    "ridge_pipeline_best_y2 = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"ridge\", Ridge())\n",
    "])\n",
    "ridge_pipeline_best_y2.set_params(**best_ridge_y2)\n",
    "\n",
    "plot_learning_curves(ridge_pipeline_best_y2, X_train, y2_train, \"Ridge (Best) - Y2\")\n",
    "plt.savefig(os.path.join(\"outputs\", \"learning_curve_ridge_y2.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "best_xgb_y1 = get_best_params(xgb_results_df[xgb_results_df[\"target\"]==\"Y1\"])\n",
    "xgb_pipeline_best_y1 = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"xgb\", XGBRegressor(objective=\"reg:squarederror\"))\n",
    "])\n",
    "xgb_pipeline_best_y1.set_params(**best_xgb_y1)\n",
    "\n",
    "plot_learning_curves(xgb_pipeline_best_y1, X_train, y1_train, \"XGBoost (Best) - Y1\")\n",
    "plt.savefig(os.path.join(\"outputs\", \"learning_curve_xgb_y1.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "best_xgb_y2 = get_best_params(xgb_results_df[xgb_results_df[\"target\"]==\"Y2\"])\n",
    "xgb_pipeline_best_y2 = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"xgb\", XGBRegressor(objective=\"reg:squarederror\"))\n",
    "])\n",
    "xgb_pipeline_best_y2.set_params(**best_xgb_y2)\n",
    "\n",
    "plot_learning_curves(xgb_pipeline_best_y2, X_train, y2_train, \"XGBoost (Best) - Y2\")\n",
    "plt.savefig(os.path.join(\"outputs\", \"learning_curve_xgb_y2.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"All model results saved to 'ridge_results.csv' and 'xgb_results.csv'.\")\n",
    "print(\"Learning curves saved to 'outputs/'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Results Overview\n",
    "\n",
    "I recorded **every** parameter combination’s average cross-validation MSE and test-set MSE in two CSV files:\n",
    "- [`ridge_results.csv`](outputs/ridge_results.csv)  \n",
    "- [`xgb_results.csv`](outputs/xgb_results.csv)  \n",
    "\n",
    "These files list each tried hyperparameter setting, along with the corresponding errors. Checking them out lets you see which values of $\\alpha$ or tree depth worked best.\n",
    "\n",
    "### 5. Learning Curves\n",
    "\n",
    "I also plotted learning curves for the best-performing parameter settings on each model and target:\n",
    "\n",
    "![Ridge (Best) - Y1](outputs/learning_curve_ridge_y1.png)  \n",
    "![Ridge (Best) - Y2](outputs/learning_curve_ridge_y2.png)  \n",
    "![XGBoost (Best) - Y1](outputs/learning_curve_xgb_y1.png)  \n",
    "![XGBoost (Best) - Y2](outputs/learning_curve_xgb_y2.png)\n",
    "\n",
    "These graphs display **Training MSE** and **Validation MSE** as the training set size grows. A narrow gap between these lines usually indicates that the model generalizes well and isn’t overly complex.\n",
    "\n",
    "### 6. Interpretation and Next Steps\n",
    "\n",
    "- **Ridge Regression**: Offers a more straightforward, linear way to capture relationships. Its MSE is moderate is not good though. This shows that while some linear patterns exist, the problem likely has nonlinear relationships. I don't recommend using this model. \n",
    "- **XGBoost**: Tends to produce substantially lower errors for both heating (Y1) and cooling (Y2) tasks. This is the model I would recommend. It can handle nonlinear interactions between building parameters.\n",
    "\n",
    "If you want a model that’s easier to interpret, a linear approach like Ridge may be appropriate. However, XGBoost is a better choice. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
